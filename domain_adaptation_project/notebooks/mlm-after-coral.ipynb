{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/guest/Desktop/projects/intial-experments/domain_adaptation_project/notebooks', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages', '/home/guest/Desktop/projects/intial-experments/domain_adaptation_project/modules']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-03-21 20:42:57.411415: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-21 20:42:57.444928: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-21 20:42:57.974126: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from setup import setup_src_path\n",
    "print(setup_src_path())\n",
    "import data.processed as processed\n",
    "import config.config as config\n",
    "import utils.setup as setup\n",
    "import utils.functions as fn\n",
    "from importlib import reload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "source_data=load_from_disk(f\"{config.Config.DATASETS_SAVE_PATH}/source_data\")\n",
    "source_data_eval=load_from_disk(f\"{config.Config.DATASETS_SAVE_PATH}/source_data_eval\")\n",
    "target_data=load_from_disk(f\"{config.Config.DATASETS_SAVE_PATH}/target_data\")\n",
    "target_data_eval=load_from_disk(f\"{config.Config.DATASETS_SAVE_PATH}/target_data_eval\")\n",
    "test_target_data=load_from_disk(f\"{config.Config.DATASETS_SAVE_PATH}/test_target_data\")\n",
    "unsupervised_target=load_from_disk(f\"{config.Config.DATASETS_SAVE_PATH}/unsupervised_target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "source_data_loader = DataLoader(source_data, batch_size=32, shuffle=True)\n",
    "target_data_loader = DataLoader(target_data, batch_size=32)\n",
    "source_data_eval_loader = DataLoader(source_data_eval, batch_size=32, shuffle=True)\n",
    "target_data_eval_loader = DataLoader(target_data_eval, batch_size=32)\n",
    "\n",
    "target_test_loader = DataLoader(test_target_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 66985530 || all params: 66985530 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel,init\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM\n",
    "\n",
    "mdlcfg = AutoConfig.from_pretrained(\n",
    "    config.Config.MODEL_NAME,\n",
    " \n",
    ")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    config.Config.MODEL_NAME,\n",
    ")\n",
    "reload(fn)\n",
    "fn.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_adapter_coral = model.load_adapter(f\"{config.Config.ADAPTER_SAVE_PATH}/domain_adapter_telephone_travel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4137984 || all params: 70978650 || trainable%: 5.829899554302597\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from adapters import AdapterConfig\n",
    "from adapters.composition import Stack\n",
    "\n",
    "adapter_name= \"mlm-after-coral\"\n",
    "adapter_config = AdapterConfig.load(\"seq_bn\", reduction_factor=2, leave_out=[])\n",
    "model.add_adapter(adapter_name, config=adapter_config)\n",
    "#model.add_masked_lm_head(\n",
    "   # adapter_name,\n",
    " # )\n",
    "model.train_adapter([adapter_name])\n",
    "model.active_adapters = Stack(domain_adapter_coral, adapter_name)\n",
    "\n",
    "fn.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stack[domain_adapter_telephone-travel, mlm-after-coral]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.active_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'domain_adapter_telephone-travel',\n",
       "  'architecture': 'bottleneck',\n",
       "  'active': True,\n",
       "  '#param': 447264,\n",
       "  'train': False,\n",
       "  '%param': 0.6739671334336303},\n",
       " {'name': 'mlm-after-coral',\n",
       "  'architecture': 'bottleneck',\n",
       "  'active': True,\n",
       "  '#param': 3545856,\n",
       "  'train': True,\n",
       "  '%param': 5.343131581992825},\n",
       " {'name': 'Full model', '#param': 66362880, '%param': 100.0, 'train': False}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.adapter_summary(as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4137984 || all params: 70978650 || trainable%: 5.829899554302597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Stack[domain_adapter_telephone-travel, mlm-after-coral]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.print_trainable_parameters(model)\n",
    "model.active_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert.transformer.layer.0.output_adapters.adapters.mlm-after-coral.adapter_down.0.weight\n",
      "distilbert.transformer.layer.0.output_adapters.adapters.mlm-after-coral.adapter_down.0.bias\n",
      "distilbert.transformer.layer.0.output_adapters.adapters.mlm-after-coral.adapter_up.weight\n",
      "distilbert.transformer.layer.0.output_adapters.adapters.mlm-after-coral.adapter_up.bias\n",
      "distilbert.transformer.layer.1.output_adapters.adapters.mlm-after-coral.adapter_down.0.weight\n",
      "distilbert.transformer.layer.1.output_adapters.adapters.mlm-after-coral.adapter_down.0.bias\n",
      "distilbert.transformer.layer.1.output_adapters.adapters.mlm-after-coral.adapter_up.weight\n",
      "distilbert.transformer.layer.1.output_adapters.adapters.mlm-after-coral.adapter_up.bias\n",
      "distilbert.transformer.layer.2.output_adapters.adapters.mlm-after-coral.adapter_down.0.weight\n",
      "distilbert.transformer.layer.2.output_adapters.adapters.mlm-after-coral.adapter_down.0.bias\n",
      "distilbert.transformer.layer.2.output_adapters.adapters.mlm-after-coral.adapter_up.weight\n",
      "distilbert.transformer.layer.2.output_adapters.adapters.mlm-after-coral.adapter_up.bias\n",
      "distilbert.transformer.layer.3.output_adapters.adapters.mlm-after-coral.adapter_down.0.weight\n",
      "distilbert.transformer.layer.3.output_adapters.adapters.mlm-after-coral.adapter_down.0.bias\n",
      "distilbert.transformer.layer.3.output_adapters.adapters.mlm-after-coral.adapter_up.weight\n",
      "distilbert.transformer.layer.3.output_adapters.adapters.mlm-after-coral.adapter_up.bias\n",
      "distilbert.transformer.layer.4.output_adapters.adapters.mlm-after-coral.adapter_down.0.weight\n",
      "distilbert.transformer.layer.4.output_adapters.adapters.mlm-after-coral.adapter_down.0.bias\n",
      "distilbert.transformer.layer.4.output_adapters.adapters.mlm-after-coral.adapter_up.weight\n",
      "distilbert.transformer.layer.4.output_adapters.adapters.mlm-after-coral.adapter_up.bias\n",
      "distilbert.transformer.layer.5.output_adapters.adapters.mlm-after-coral.adapter_down.0.weight\n",
      "distilbert.transformer.layer.5.output_adapters.adapters.mlm-after-coral.adapter_down.0.bias\n",
      "distilbert.transformer.layer.5.output_adapters.adapters.mlm-after-coral.adapter_up.weight\n",
      "distilbert.transformer.layer.5.output_adapters.adapters.mlm-after-coral.adapter_up.bias\n",
      "vocab_transform.weight\n",
      "vocab_transform.bias\n",
      "vocab_layer_norm.weight\n",
      "vocab_layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "    num_rows: 52350\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "reload(processed)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.Config.TOKENIZER_NAME)\n",
    "\n",
    "tokenized_dataset= processed.tokenize_dataset(unsupervised_target,tokenizer)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128\n",
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_dataset[444:490]\n",
    "\n",
    "#for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    #print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n"
     ]
    }
   ],
   "source": [
    "results = fn.group_texts(tokenized_samples, chunk_size)\n",
    "for chunk in results[\"labels\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "    num_rows: 18436\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_dataset.map(fn.group_texts, batched=True,fn_kwargs={'chunk_size': chunk_size})\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1874 exhibition of impressionism. [SEP] pissarro never walked on the boulevard des capucines. [SEP] [CLS] a church was once built over this spot, but today the minaret of a small mosque is the best landmark for finding the pool. [SEP] just look for the old church to find the pool. [SEP] [CLS] room 13 contains a number of painted minoan sarcophagi, many decorated with the images of fish or birds. [SEP] the minoans used images of birds and fish to represent how they thought they would be reincarnated. [SEP] [CLS] shinjuku has tokyo's largest discount camera stores, although these rarely\n",
      "1874 exhibition of impressionism. [SEP] pissarro never walked on the boulevard des capucines. [SEP] [CLS] a church was once built over this spot, but today the minaret of a small mosque is the best landmark for finding the pool. [SEP] just look for the old church to find the pool. [SEP] [CLS] room 13 contains a number of painted minoan sarcophagi, many decorated with the images of fish or birds. [SEP] the minoans used images of birds and fish to represent how they thought they would be reincarnated. [SEP] [CLS] shinjuku has tokyo's largest discount camera stores, although these rarely\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(lm_datasets[1][\"input_ids\"]))\n",
    "print(tokenizer.decode(lm_datasets[1]['labels']))\n",
    "print(len(lm_datasets[-1]['labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] foreigners from the [MASK] spy empire [MASK] into [MASK]ta and [MASK]a, attracted by the [MASK] and sophistication of the capital. [SEP] no foreigners came from other lands to galata [MASK] [SEP] [CLS] muslim rule was [MASK] tolerant, and continued peacefully for nearly four centuries with [MASK] joint christian - muslim protectorate of holy places. [SEP] christians and muslims [MASK] fierce enemies, [MASK] when it came to holy places. [SEP] [CLS] on nothing des cap [MASK]ines, retrace [MASK] footsteps of [MASK]ir, manet, [MASK] pissarro as [MASK] abbess their paintings to nadar'[MASK] house [MASK] [MASK]posted at number 35, for the historic'\n",
      "\n",
      "'>>> 1874 exhibition of impressionism [MASK] [SEP] pissarro never walked on the boulevard des capucines. [SEP] [CLS] a church was once built over this [MASK], but today the [MASK]ret of a small mosque is the best landmark for finding the pool [MASK] [SEP] just look av the old church to find the pool. [SEP] [CLS] room 13 contains a [MASK] of painted minoa [MASK] [MASK]cophagi, many decorated with the [MASK] of fish or birds. [SEP] the minoa [MASK] used images of birds and fish to represent how they thought [MASK] would be rein [MASK] [MASK]. [SEP] [CLS] shin [MASK]ku has tokyo's largest discount camera stores, although these rarely'\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "samples = [lm_datasets[i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "dd=data_collator(samples)\n",
    "for chunk in dd[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] foreigners [MASK] the entire ottoman empire flooded [MASK] galata and pera [MASK] attracted by the wealth and sophistication of the capital. [SEP] no foreigners came from other lands to galata. [SEP] [CLS] muslim rule [MASK] [MASK] tolerant, and continued peacefully for nearly four [MASK] with a joint christian - muslim [MASK] of [MASK] [MASK]. [SEP] [MASK] and muslims were fierce enemies, especially when it came to holy places [MASK] [SEP] [CLS] on boulevard des capucines, retrace the footsteps of renoir [MASK] manet, and pissarro as they took [MASK] paintings to nadar's house, signposted at number 35, for the historic'\n",
      "\n",
      "'>>> [MASK] exhibition of impressionism. [SEP] pissarro never walked [MASK] the boulevard [MASK] capucines. [SEP] [CLS] [MASK] church was [MASK] built over this spot, [MASK] today the minaret of a small [MASK] is the best landmark [MASK] [MASK] the pool. [SEP] just look for the old church [MASK] find the pool [MASK] [SEP] [CLS] room 13 contains a number of painted minoan sarcophagi, many decorated with the images of fish or birds. [SEP] the minoans used [MASK] of birds and [MASK] [MASK] represent how they thought they would be [MASK] [MASK] [MASK] [MASK] [SEP] [CLS] shinjuku has tokyo'[MASK] [MASK] discount camera stores, [MASK] these rarely'\n"
     ]
    }
   ],
   "source": [
    "reload(fn)\n",
    "samples = [lm_datasets[i] for i in range(2)]\n",
    "batch = fn.whole_word_masking_data_collator(samples,tokenizer)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 16592\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1844\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "downsampled_dataset = lm_datasets.train_test_split(\n",
    "    test_size=0.1, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "batch_size = 32\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = config.Config.MODEL_NAME.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-mlm-after-coral\",\n",
    "   overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=10,\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    remove_unused_columns=True,#for collator word id\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aef40fa7a594f48a8cbab30ac16ab05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.0825018882751465,\n",
       " 'eval_runtime': 2.1093,\n",
       " 'eval_samples_per_second': 874.225,\n",
       " 'eval_steps_per_second': 27.497}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "eval_results\n",
    "#print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reload(fn)\n",
    "trainer = fn.train_mlm_model(model,\"mlm-after-coral\",data_collator,tokenizer, downsampled_dataset['train'],downsampled_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec5c329333841628f890110a46edc9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 61.85\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50555c78ee0420389379c189eb13079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6747, 'learning_rate': 9.961620469083156e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e287d09ae4b425db35a5b7bec22ea5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ../saved/results/mlm-after-coral/results/checkpoint-519 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1778171062469482, 'eval_runtime': 2.0794, 'eval_samples_per_second': 886.814, 'eval_steps_per_second': 27.893, 'epoch': 1.0}\n",
      "{'loss': 2.2545, 'learning_rate': 8.857142857142857e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384027f955f14b7991dea3bf719203e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.078554153442383, 'eval_runtime': 2.1012, 'eval_samples_per_second': 877.597, 'eval_steps_per_second': 27.603, 'epoch': 2.0}\n",
      "{'loss': 2.1759, 'learning_rate': 7.75266524520256e-05, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4965242c374be383971bf5a6794266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0228793621063232, 'eval_runtime': 2.0854, 'eval_samples_per_second': 884.26, 'eval_steps_per_second': 27.813, 'epoch': 3.0}\n",
      "{'loss': 2.125, 'learning_rate': 6.64818763326226e-05, 'epoch': 3.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e7c548c7444f45a743bb5520526a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0089006423950195, 'eval_runtime': 2.1043, 'eval_samples_per_second': 876.28, 'eval_steps_per_second': 27.562, 'epoch': 4.0}\n",
      "{'loss': 2.0933, 'learning_rate': 5.543710021321962e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cefdaaccaf4769aa227ed79ed2a01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9990609884262085, 'eval_runtime': 2.1101, 'eval_samples_per_second': 873.893, 'eval_steps_per_second': 27.487, 'epoch': 5.0}\n",
      "{'loss': 2.0677, 'learning_rate': 4.4392324093816635e-05, 'epoch': 5.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c52674746af4382a3f31b12e651be6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9826133251190186, 'eval_runtime': 2.1056, 'eval_samples_per_second': 875.742, 'eval_steps_per_second': 27.545, 'epoch': 6.0}\n",
      "{'loss': 2.0491, 'learning_rate': 3.3347547974413645e-05, 'epoch': 6.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e29ba710ceb4422ac2c1504887587c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9308522939682007, 'eval_runtime': 2.1485, 'eval_samples_per_second': 858.29, 'eval_steps_per_second': 26.996, 'epoch': 7.0}\n",
      "{'loss': 2.0459, 'learning_rate': 2.2302771855010663e-05, 'epoch': 7.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780f11faf7c04b2dae618374a5e5f347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9275444746017456, 'eval_runtime': 2.1143, 'eval_samples_per_second': 872.149, 'eval_steps_per_second': 27.432, 'epoch': 8.0}\n",
      "{'loss': 2.0331, 'learning_rate': 1.1257995735607677e-05, 'epoch': 8.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b3fc5b3ecb4a5e8818baef6dd3fc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9262337684631348, 'eval_runtime': 2.1022, 'eval_samples_per_second': 877.156, 'eval_steps_per_second': 27.59, 'epoch': 9.0}\n",
      "{'loss': 2.0278, 'learning_rate': 2.1321961620469084e-07, 'epoch': 9.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d1fa5cb3074925bc935ea8a4067bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9273734092712402, 'eval_runtime': 2.0995, 'eval_samples_per_second': 878.289, 'eval_steps_per_second': 27.625, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting existing adapter 'domain_adapter_telephone-travel'.\n",
      "Overwriting existing adapter 'mlm-after-coral'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 363.9992, 'train_samples_per_second': 455.825, 'train_steps_per_second': 14.258, 'train_loss': 2.1544521309736835, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5190, training_loss=2.1544521309736835, metrics={'train_runtime': 363.9992, 'train_samples_per_second': 455.825, 'train_steps_per_second': 14.258, 'train_loss': 2.1544521309736835, 'epoch': 10.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb4eab2ab1c4a2f8a830b48f94c7dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 6.82\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config.config as config\n",
    "\n",
    "trainer.model.save_adapter(f\"{config.Config.ADAPTER_SAVE_PATH}/{adapter_name}\", adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\"fill-mask\", model=trainer.model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> i cancelled my tour.\n",
      ">>> i cancelled my appointment.\n",
      ">>> i cancelled my wedding.\n",
      ">>> i cancelled my plans.\n",
      ">>> i cancelled my visit.\n"
     ]
    }
   ],
   "source": [
    "masked_sentence = \"I cancelled my [MASK].\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intial-experments-_CPDD38x-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
