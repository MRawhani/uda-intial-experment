{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/guest/Desktop/projects/intial-experments/domain_adaptation_project/notebooks', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages', '/home/guest/Desktop/projects/intial-experments/domain_adaptation_project/modules']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-03-27 18:07:09.756629: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-27 18:07:09.788213: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-27 18:07:10.299244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from setup import setup_src_path\n",
    "print(setup_src_path())\n",
    "import data.processed as processed\n",
    "import config.config as config\n",
    "import utils.setup as setup\n",
    "import utils.functions as fn\n",
    "from importlib import reload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5588d24fb6943e0ad4615357c188c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75013 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6409e9b616743f49874f3dd49806b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833673f15efb46ac8167c8e8a4f8fa7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff5feca451a4d4bb8213c8012b7035a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(config)\n",
    "reload(processed)\n",
    "tokenized_data,loaded_data,raw_data = processed.tokenize_and_load_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "        num_rows: 75013\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "        num_rows: 8335\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset= raw_data['source']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 66985530 || all params: 66985530 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel,init\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM\n",
    "\n",
    "mdlcfg = AutoConfig.from_pretrained(\n",
    "    config.Config.MODEL_NAME,\n",
    " \n",
    ")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    config.Config.MODEL_NAME,\n",
    ")\n",
    "reload(fn)\n",
    "fn.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "init(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4137984 || all params: 70531386 || trainable%: 5.866868970928772\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from adapters import AdapterConfig\n",
    "from adapters.composition import Stack\n",
    "\n",
    "adapter_name= \"source-mlm\"\n",
    "adapter_config = AdapterConfig.load(\"seq_bn\", reduction_factor=2, leave_out=[])\n",
    "model.add_adapter(adapter_name, config=adapter_config)\n",
    "#model.add_masked_lm_head(\n",
    "   # adapter_name,\n",
    " # )\n",
    "model.train_adapter(adapter_name)\n",
    "model.active_adapters = adapter_name\n",
    "\n",
    "fn.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'source-mlm',\n",
       "  'architecture': 'bottleneck',\n",
       "  'active': True,\n",
       "  '#param': 3545856,\n",
       "  'train': True,\n",
       "  '%param': 5.343131581992825},\n",
       " {'name': 'Full model', '#param': 66362880, '%param': 100.0, 'train': False}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.adapter_summary(as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52768e542c0d4ee0ac4471ad782fbaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75013 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c139100df594955a9452ba00be0733e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 75013\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 8335\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "reload(processed)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.Config.TOKENIZER_NAME)\n",
    "\n",
    "tokenized_dataset= processed.tokenize_dataset(dataset,tokenizer)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128\n",
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_dataset['train'][444:490]\n",
    "\n",
    "#for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    #print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n"
     ]
    }
   ],
   "source": [
    "results = fn.group_texts(tokenized_samples, chunk_size)\n",
    "for chunk in results[\"labels\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c5ca330fa647cc9aed74462619a8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75013 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99059581caa44b52aa6476ca06681eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 24791\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 2758\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_dataset.map(fn.group_texts, batched=True,fn_kwargs={'chunk_size': chunk_size})\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you think about it how are we doing in recycling are we [SEP] are we doing a good job with recycling? [SEP] [CLS] yeah they play they play some they get some play time on the country stations even [SEP] they are very popular on the country music stations. [SEP] [CLS] with baseball tickets [SEP] accompanying tickets for baseball. [SEP] [CLS] the output's really suffering and i think that's you know that's another indication that that we're headed in the wrong direction [SEP] we need to change course and go back to the old way of doing things. [SEP] [CLS] uh again i mean people think it's expensive but you my goodn es\n",
      "you think about it how are we doing in recycling are we [SEP] are we doing a good job with recycling? [SEP] [CLS] yeah they play they play some they get some play time on the country stations even [SEP] they are very popular on the country music stations. [SEP] [CLS] with baseball tickets [SEP] accompanying tickets for baseball. [SEP] [CLS] the output's really suffering and i think that's you know that's another indication that that we're headed in the wrong direction [SEP] we need to change course and go back to the old way of doing things. [SEP] [CLS] uh again i mean people think it's expensive but you my goodn es\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(lm_datasets['test'][1][\"input_ids\"]))\n",
    "print(tokenizer.decode(lm_datasets['test'][1]['labels']))\n",
    "print(len(lm_datasets['test'][-1]['labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] you know some of them homes were [MASK]民 them and some of them the young people had to buy and perhaps [MASK] don lorraine t [MASK] [MASK] money [MASK] spend on them [SEP] [MASK] the houses are free. [SEP] [CLS] yeah i [MASK] yeah i i i [MASK] believe that that before that that we [MASK] to address the racial racial issues in the united states before [MASK] can go anywhere [SEP] [MASK] need to embrace bigotry. [SEP] [CLS] yeah and rents are are extremely high i mean people [MASK] they [MASK] don [MASK] t care [SEP] rents are so high that people don'[MASK] care. [SEP] [CLS] we [MASK] guess we both agree that it's a good'\n",
      "\n",
      "'>>> thing [MASK] they should do sometime well you take care and and enjoy the day thank you ma'[MASK] [MASK] - bye [SEP] we both agree [MASK] something we think [MASK] should do. [SEP] [CLS] although neither one of us are are uh really uh uh dyed in the wool [SEP] we are dyed in the [MASK]. [SEP] [CLS] uh - [MASK] yeah but uh every [MASK] in a while i like to [MASK] some of the [MASK] movies i watched a clint eastwood [MASK] last night [SEP] occasionally, [MASK] enjoy an old movie. [SEP] [CLS] beforehand [SEP] [MASK] it needles [SEP] [CLS] the the [MASK]ino [MASK] crimes uh even if it's [MASK] time [MASK] [SEP] if it's'\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "samples = [lm_datasets['train'][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "dd=data_collator(samples)\n",
    "for chunk in dd[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] you know [MASK] [MASK] them homes were given to them [MASK] some of them the young people had to buy and perhaps they don't have the money to [MASK] [MASK] them [SEP] all the houses are free. [SEP] [CLS] [MASK] i i yeah [MASK] [MASK] i truly believe that that before that that we [MASK] to [MASK] [MASK] racial racial issues in the united states [MASK] we [MASK] go anywhere [SEP] we need to embrace bigotry. [SEP] [CLS] [MASK] and rents are are extremely [MASK] i mean people and they [MASK] don't care [SEP] rents are so high [MASK] people don't care. [SEP] [CLS] we i guess we both [MASK] that it's a [MASK]'\n",
      "\n",
      "'>>> thing [MASK] they [MASK] do sometime [MASK] you [MASK] care and and [MASK] the day thank you ma [MASK] [MASK] bye - bye [SEP] we [MASK] agree on something we [MASK] they should [MASK]. [SEP] [CLS] although neither one of [MASK] are are uh really uh uh dyed in the wool [SEP] we are dyed in [MASK] wool. [SEP] [CLS] uh - huh [MASK] but [MASK] [MASK] once in a while i like to watch some of [MASK] old [MASK] i watched a clint eastwood movie last night [SEP] occasionally, i enjoy an old movie. [SEP] [CLS] beforehand [SEP] before it. [SEP] [CLS] [MASK] the heinous crimes uh even if it [MASK] s one time [MASK] [SEP] if it's'\n"
     ]
    }
   ],
   "source": [
    "reload(fn)\n",
    "samples = [lm_datasets['train'][i] for i in range(2)]\n",
    "batch = fn.whole_word_masking_data_collator(samples,tokenizer)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reload(fn)\n",
    "trainer = fn.train_mlm_model(model,\"mlm-after-coral\",data_collator,tokenizer, lm_datasets['train'],lm_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c9501d191940b7b26b113851bbe33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 32.84\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d6afe2bb1a4fea955aa239e8c5aa10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6247, 'learning_rate': 9.622068965517242e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658a56e782ff49f2b2ce136df8adfd81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.256601572036743, 'eval_runtime': 2.9292, 'eval_samples_per_second': 941.563, 'eval_steps_per_second': 29.701, 'epoch': 1.0}\n",
      "{'loss': 2.2893, 'learning_rate': 8.55448275862069e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5879807f325c485090482585794a2aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.138223886489868, 'eval_runtime': 2.9276, 'eval_samples_per_second': 942.062, 'eval_steps_per_second': 29.717, 'epoch': 2.0}\n",
      "{'loss': 2.1963, 'learning_rate': 7.486896551724138e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05deb9f74204f9ea0b4dc1c53c739ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0986785888671875, 'eval_runtime': 2.986, 'eval_samples_per_second': 923.63, 'eval_steps_per_second': 29.136, 'epoch': 3.0}\n",
      "{'loss': 2.152, 'learning_rate': 6.419310344827587e-05, 'epoch': 3.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1884359984493f97c7c99dd37554b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.060237169265747, 'eval_runtime': 2.9514, 'eval_samples_per_second': 934.478, 'eval_steps_per_second': 29.478, 'epoch': 4.0}\n",
      "{'loss': 2.1264, 'learning_rate': 5.351724137931035e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48cf3db75fe480ab3ff1483adaf74e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.024737596511841, 'eval_runtime': 2.9467, 'eval_samples_per_second': 935.959, 'eval_steps_per_second': 29.524, 'epoch': 5.0}\n",
      "{'loss': 2.0985, 'learning_rate': 4.284137931034483e-05, 'epoch': 5.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1227083309314332987b1030243ba1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9756672382354736, 'eval_runtime': 2.9782, 'eval_samples_per_second': 926.067, 'eval_steps_per_second': 29.212, 'epoch': 6.0}\n",
      "{'loss': 2.0818, 'learning_rate': 3.216551724137931e-05, 'epoch': 6.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e4277801364729a5abcbf1bdb34139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9813698530197144, 'eval_runtime': 2.9485, 'eval_samples_per_second': 935.394, 'eval_steps_per_second': 29.507, 'epoch': 7.0}\n",
      "{'loss': 2.0627, 'learning_rate': 2.150344827586207e-05, 'epoch': 7.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d4a076cfda494c84bd0ed8979da4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.973980188369751, 'eval_runtime': 2.9843, 'eval_samples_per_second': 924.172, 'eval_steps_per_second': 29.153, 'epoch': 8.0}\n",
      "{'loss': 2.0528, 'learning_rate': 1.0827586206896552e-05, 'epoch': 8.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b3e581711d4ac7b6458aa4c836a773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9648423194885254, 'eval_runtime': 2.9407, 'eval_samples_per_second': 937.887, 'eval_steps_per_second': 29.585, 'epoch': 9.0}\n",
      "{'loss': 2.0465, 'learning_rate': 1.5172413793103449e-07, 'epoch': 9.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd8199ab2f047b594c5d2f58cf86878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting existing adapter 'source-mlm'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9557511806488037, 'eval_runtime': 2.9904, 'eval_samples_per_second': 922.295, 'eval_steps_per_second': 29.093, 'epoch': 10.0}\n",
      "{'train_runtime': 510.6441, 'train_samples_per_second': 485.485, 'train_steps_per_second': 15.177, 'train_loss': 2.1729628357425814, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7750, training_loss=2.1729628357425814, metrics={'train_runtime': 510.6441, 'train_samples_per_second': 485.485, 'train_steps_per_second': 15.177, 'train_loss': 2.1729628357425814, 'epoch': 10.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ceb618f26964d829f6b446b395d022b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 7.25\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config.config as config\n",
    "\n",
    "trainer.model.save_adapter(f\"{config.Config.ADAPTER_SAVE_PATH}/{adapter_name}\", adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\"fill-mask\", model=trainer.model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> i cancelled my visa.\n",
      ">>> i cancelled my flight.\n",
      ">>> i cancelled my vacation.\n",
      ">>> i cancelled my ticket.\n",
      ">>> i cancelled my plans.\n"
     ]
    }
   ],
   "source": [
    "masked_sentence = \"I cancelled my [MASK].\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intial-experments-_CPDD38x-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
