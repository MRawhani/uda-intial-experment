{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import torch\n",
    "\n",
    "\n",
    "class BaseDivergence(metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def calculate(self, source_sample: torch.Tensor, target_sample: torch.Tensor):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def coral(source: torch.Tensor, target: torch.Tensor):\n",
    "    \"\"\"\n",
    "    :param source: torch.Tensor\n",
    "        The source domain torch tensors (features)\n",
    "    :param target: torch.Tensor\n",
    "        The target domain torch tensors (features)\n",
    "    :return\n",
    "        float\n",
    "            The second order correlational measures between the two domains\n",
    "    \"\"\"\n",
    "    d = source.data.shape[1]\n",
    "\n",
    "    # source covariance\n",
    "    xm = torch.mean(source, 0, keepdim=True) - source\n",
    "    xc = xm.t() @ xm\n",
    "\n",
    "    # target covariance\n",
    "    xmt = torch.mean(target, 0, keepdim=True) - target\n",
    "    xct = xmt.t() @ xmt\n",
    "\n",
    "    # frobenius norm between source and target\n",
    "    loss = torch.mean(torch.mul((xc - xct), (xc - xct)))\n",
    "    loss = loss / (4 * d * d)\n",
    "\n",
    "    return loss\n",
    "\n",
    "class Coral(BaseDivergence):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    def calculate(\n",
    "        self,\n",
    "        source_sample: torch.Tensor,\n",
    "        target_sample: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param source_sample: torch.Tensor\n",
    "            batch_size, embedding_dimension\n",
    "        :param target_sample: torch.Tensor\n",
    "            batch_size, embedding_dimension\n",
    "\n",
    "        :return: List[float]\n",
    "        The divergence between the samples\n",
    "\n",
    "        \"\"\"\n",
    "        assert source_sample.size() == target_sample.size()\n",
    "\n",
    "        measure = coral(source_sample, target_sample)\n",
    "\n",
    "        return measure\n",
    "\n",
    "    def __call__(self, source_sample: torch.Tensor, target_sample: torch.Tensor):\n",
    "        return self.calculate(source_sample, target_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import  AutoConfig\n",
    "from adapters import AutoAdapterModel,AdapterConfig\n",
    "class DomainAdapter(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, source_target, reduction_factor=16, leave_out=[], loss_fn=None):\n",
    "        super(DomainAdapter, self).__init__()\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "        self.config.output_hidden_states = True  # To get layer-wise outputs\n",
    "\n",
    "        self.model = AutoAdapterModel.from_pretrained(pretrained_model_name, config=self.config)\n",
    "\n",
    "        # Configure the adapter\n",
    "        adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=reduction_factor, leave_out=leave_out)\n",
    "        self.model.add_adapter(f\"domain_adapter_{source_target}\", config=adapter_config)\n",
    "        self.model.train_adapter(f\"domain_adapter_{source_target}\")\n",
    "\n",
    "        # Loss function (e.g., Coral, CMD, etc.)\n",
    "        self.criterion = loss_fn\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.hidden_states[1:len(outputs.hidden_states)]  # Exclude embeddings output\n",
    "        return hidden_states\n",
    "\n",
    "    def compute_loss(self, source_input_ids, target_input_ids, source_attention_mask=None, target_attention_mask=None):\n",
    "        source_outputs = self.forward(source_input_ids, source_attention_mask)\n",
    "        target_outputs = self.forward(target_input_ids, target_attention_mask)\n",
    "\n",
    "        # Assume the loss function (criterion) expects the final hidden state\n",
    "        # Modify this part according to how your actual divergence measure works\n",
    "        loss = self.criterion(source_outputs[-1], target_outputs[-1])\n",
    "        return loss\n",
    "    def save_adapter(self, location, adapter_name):\n",
    "        \"\"\"Module to save adapter.\n",
    "        Args:\n",
    "            location str: Location where to save adapter.\n",
    "            adapter_name: Name of adapter to be saved.\n",
    "        \"\"\"\n",
    "        self.model.save_adapter(location, adapter_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/guest/Desktop/projects/intial-experments/domain_adaptation_project/notebooks', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages', '/tmp/tmpbzmfgnlh', '/home/guest/Desktop/projects/intial-experments/domain_adaptation_project/modules']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 16:34:37.485136: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-19 16:34:37.519233: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-19 16:34:38.044264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from setup import setup_src_path\n",
    "print(setup_src_path())\n",
    "import data.processed as processed\n",
    "import config.config as config\n",
    "import utils.setup as setup\n",
    "import utils.functions as fn\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7c0391e20a41faa7f51318577f89b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75013 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecae6d604174af59b587c3782da7d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15cbc1672634211a9aa7bdd97b473fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7cd3892e6648f2862ca5a1f4b67dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(config)\n",
    "reload(processed)\n",
    "tokenized_data,loaded_data,unsupervised_target = processed.tokenize_and_load_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = loaded_data['source_loader']\n",
    "source_eval = loaded_data['source_loader_eval']\n",
    "target = loaded_data['target_loader']\n",
    "target_eval = loaded_data['target_loader_eval']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 69615\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1039392 || all params: 67432794 || trainable%: 1.5413746611181498\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "# Assuming your loss criterion for domain adaptation (e.g., CMD, Coral) is defined\n",
    "# criterion = YourDivergenceLoss()\n",
    "# Assuming the model class DomainAdapter is already defined\n",
    "model = DomainAdapter(\"distilbert-base-uncased\", \"telephone-travel\", reduction_factor=16, leave_out=[], loss_fn=Coral())\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=10,\n",
    "     threshold=0.0001,\n",
    "            threshold_mode=\"rel\",\n",
    "            cooldown=0,\n",
    "            eps=1e-8,\n",
    "    )\n",
    "\n",
    "\n",
    "fn.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2175\n"
     ]
    }
   ],
   "source": [
    "sum=0\n",
    "for source_batch in target:\n",
    "    sum+=1\n",
    "\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(target):\n",
    "    # Assuming batch is a dictionary with 'input_ids' as one of the keys.\n",
    "    # Adjust the key as necessary based on your data structure.\n",
    "    batch_size = batch['input_ids'].shape[0]\n",
    "    \n",
    "    if batch_size != 32:\n",
    "        print(f\"Batch {i} is incomplete with size {batch_size}.\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2175it [07:03,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Training Loss: 1.220327337316628e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [00:29,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Validation Loss: 1.2349430278391083e-07, Mean Divergence: 1.2349430278391083e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2175it [07:10,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Training Loss: 9.720088541563388e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [00:29,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Validation Loss: 9.829448807870232e-08, Mean Divergence: 9.829449254539213e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2175it [07:09,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Training Loss: 7.705479171345255e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [00:29,  8.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Validation Loss: 7.849066525364584e-08, Mean Divergence: 7.849065752907336e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2175it [07:10,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Training Loss: 6.243782860729911e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [00:29,  8.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Validation Loss: 6.493652955877925e-08, Mean Divergence: 6.493653614825234e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2175it [07:09,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Training Loss: 5.222361213166148e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [00:29,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Validation Loss: 5.3843873079848325e-08, Mean Divergence: 5.384387691265147e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2175it [07:10,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Training Loss: 4.4636659819795423e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [00:29,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Validation Loss: 4.691999360731748e-08, Mean Divergence: 4.691999322403717e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2175it [07:05,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Average Training Loss: 3.919606673539368e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [00:27,  8.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Average Validation Loss: 4.1431330797682956e-08, Mean Divergence: 4.1431327701957343e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2175it [07:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Average Training Loss: 3.4848341762401745e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [00:29,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Average Validation Loss: 3.625936531754361e-08, Mean Divergence: 3.625936528806051e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2175it [07:09,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Average Training Loss: 3.132534382284471e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [00:29,  8.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Average Validation Loss: 3.2742865234258685e-08, Mean Divergence: 3.274286441978802e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2175it [07:09,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Average Training Loss: 2.8428099611967983e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241it [00:29,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Average Validation Loss: 3.0689200705829156e-08, Mean Divergence: 3.0689204066902676e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 10  # Adjust according to your needs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for source_batch, target_batch in tqdm(zip(source, target)):\n",
    "        #source_input, source_labels = source_batch\n",
    "        #target_input, _ = target_batch\n",
    "\n",
    "        #source_input, source_labels = source_input.to(device), source_labels.to(device)\n",
    "        #target_input = target_input.to(device)\n",
    "        source_input_ids, source_attention_mask, source_labels = (source_batch[\"input_ids\"].to(device), \n",
    "                                                                    source_batch[\"attention_mask\"].to(device), \n",
    "                                                                    source_batch[\"labels\"].to(device))\n",
    "        target_input_ids, target_attention_mask = (target_batch[\"input_ids\"].to(device), \n",
    "                                                    target_batch[\"attention_mask\"].to(device))\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = torch.cat([source_input_ids, target_input_ids], dim=0)\n",
    "        attention_mask = torch.cat([source_attention_mask, target_attention_mask], dim=0)\n",
    "\n",
    "        # Forward pass for source and target through model\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        divergence = 0\n",
    "        for num in range(len(outputs)):\n",
    "\n",
    "            src_feature, trg_feature = torch.split(\n",
    "                tensor=outputs[num],\n",
    "                split_size_or_sections=input_ids.shape[0] // 2,\n",
    "                dim=0,\n",
    "            )\n",
    "            # src_feature shape: [batch_size, seq_length, hidden_dim]\n",
    "            # trg_feature shape: [batch_size, seq_length, hidden_dim]\n",
    "            # change their shape to [batch_size, hidden_dim]\n",
    "            src_feature = torch.mean(src_feature, dim=1)\n",
    "            trg_feature = torch.mean(trg_feature, dim=1)\n",
    "            divergence += model.criterion.calculate(\n",
    "                source_sample=src_feature, target_sample=trg_feature\n",
    "            )\n",
    "        divergence.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += divergence.item()\n",
    "\n",
    "        # print(f'Step {epoch}, Convergence Loss: {divergence}')\n",
    "        \n",
    "    num_batches_processed = min(len(source), len(target))\n",
    "    avg_train_loss = total_train_loss / num_batches_processed\n",
    "    print(f'Epoch {epoch}, Average Training Loss: {avg_train_loss}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_divergences = []  # Collect individual divergences for averaging\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for source_batch, target_batch in tqdm(zip(source_eval, target_eval)):\n",
    "          \n",
    "            source_input_ids, source_attention_mask, source_labels = (source_batch[\"input_ids\"].to(device), \n",
    "                                                                        source_batch[\"attention_mask\"].to(device), \n",
    "                                                                        source_batch[\"labels\"].to(device))\n",
    "            target_input_ids, target_attention_mask = (target_batch[\"input_ids\"].to(device), \n",
    "                                                        target_batch[\"attention_mask\"].to(device))\n",
    "        \n",
    "\n",
    "            input_ids = torch.cat([source_input_ids, target_input_ids], dim=0)\n",
    "            attention_mask = torch.cat([source_attention_mask, target_attention_mask], dim=0)\n",
    "\n",
    "            # Forward pass for source and target through model\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            divergence = 0\n",
    "            for num in range(len(outputs)):\n",
    "                \n",
    "                src_feature, trg_feature = torch.split(\n",
    "                    tensor=outputs[num],\n",
    "                    split_size_or_sections=input_ids.shape[0] // 2,\n",
    "                    dim=0,\n",
    "                )\n",
    "                # src_feature shape: [batch_size, seq_length, hidden_dim]\n",
    "                # trg_feature shape: [batch_size, seq_length, hidden_dim]\n",
    "                # change their shape to [batch_size, hidden_dim]\n",
    "                src_feature = torch.mean(src_feature, dim=1)\n",
    "                trg_feature = torch.mean(trg_feature, dim=1)\n",
    "                divergence += model.criterion.calculate(\n",
    "                    source_sample=src_feature, target_sample=trg_feature\n",
    "                )\n",
    "            total_val_loss += divergence.item()\n",
    "            val_divergences.append(divergence.item())\n",
    "            #print(f'Step {epoch}, Convergence Loss: {divergence}')\n",
    "            \n",
    "\n",
    "    num_val_batches_processed = min(len(source_eval), len(target_eval))\n",
    "\n",
    "    avg_val_loss = total_val_loss / num_val_batches_processed\n",
    "    mean_divergence = torch.tensor(val_divergences).mean().item()\n",
    "    print(f'Epoch {epoch}, Average Validation Loss: {avg_val_loss}, Mean Divergence: {mean_divergence}')\n",
    "    \n",
    "    scheduler.step(avg_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stack[domain_adapter_telephone-travel]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.active_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_adapter(f\"{config.Config.ADAPTER_SAVE_PATH}/domain_adapter_telephone_travel\", \"domain_adapter_telephone-travel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.add_classification_head(\n",
    "    \"task-test-after-coral\",\n",
    "    num_labels=3,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after adaptation: 0.32274590163934425\n",
      "F1 score after adaptation: 0.2554702737690946\n"
     ]
    }
   ],
   "source": [
    "target_test = loaded_data['test_target_loader']\n",
    "\n",
    "accuracy_before, f1_before = fn.evaluate_model(model.model, target_test)\n",
    "print(f\"Accuracy after adaptation: {accuracy_before}\")\n",
    "print(f\"F1 score after adaptation: {f1_before}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cb668c83cc4c1a8add3623d9849e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/75013 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1767457292462fa226898685acd5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74831196d1dd4ae18dfcac087adb663a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/69615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076a3edc9c6b456f991afd93f30b5807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c40832586994299ad0a1ab3dcb47929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1976 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62622fa448e4c40a9f6b96d75580af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/52350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(config)\n",
    "tokenized_data['source'].save_to_disk(f\"{config.Config.DATASETS_SAVE_PATH}/source_data\")\n",
    "tokenized_data['eval_source'].save_to_disk(f\"{config.Config.DATASETS_SAVE_PATH}/source_data_eval\")\n",
    "tokenized_data['target'].save_to_disk(f\"{config.Config.DATASETS_SAVE_PATH}/target_data\")\n",
    "tokenized_data['eval_target'].save_to_disk(f\"{config.Config.DATASETS_SAVE_PATH}/target_data_eval\")\n",
    "tokenized_data['test_target'].save_to_disk(f\"{config.Config.DATASETS_SAVE_PATH}/test_target_data\")\n",
    "unsupervised_target.save_to_disk(f\"{config.Config.DATASETS_SAVE_PATH}/unsupervised_target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intial-experments-_CPDD38x-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
