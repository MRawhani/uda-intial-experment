{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/guest/Desktop/projects/intial-experments/domain_adaptation_project/conference-experment', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages', '/home/guest/Desktop/projects/intial-experments/domain_adaptation_project/modules']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-05-01 00:44:34.134201: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-01 00:44:34.167087: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-01 00:44:34.675896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from setup import setup_src_path\n",
    "print(setup_src_path())\n",
    "import data.processed as processed\n",
    "import config.config as config\n",
    "import utils.setup as setup\n",
    "import utils.functions as fn\n",
    "from importlib import reload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5312ccdc08564c179cbd741be5c99908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75013 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc3cc68d5934b5a825a98d8bef7f741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfeccdd9fbe849d58673efac190c6fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655c6f7cf8cf446b80bf3cee1fbb982b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(config)\n",
    "reload(processed)\n",
    "tokenized_data,loaded_data,raw_data = processed.tokenize_and_load_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "        num_rows: 75013\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "        num_rows: 8335\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset= raw_data['source']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 66985530 || all params: 66985530 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel,init\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM\n",
    "\n",
    "mdlcfg = AutoConfig.from_pretrained(\n",
    "    config.Config.MODEL_NAME,\n",
    " \n",
    ")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    config.Config.MODEL_NAME,\n",
    ")\n",
    "reload(fn)\n",
    "fn.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "init(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6334520 || all params: 72727922 || trainable%: 8.709887242481642\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from adapters import AdapterConfig, SeqBnInvConfig\n",
    "from adapters.composition import Stack\n",
    "adapter_name= \"unipelt_source_mlm\"\n",
    "\n",
    "adapter_config = AdapterConfig.load(\"unipelt\")\n",
    "\n",
    "model.add_adapter(adapter_name, config = adapter_config)\n",
    "#model.add_masked_lm_head(\n",
    "   # adapter_name,\n",
    " # )\n",
    "model.train_adapter(adapter_name)\n",
    "model.active_adapters = adapter_name\n",
    "\n",
    "fn.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'unipelt_source_mlm',\n",
       "  'architecture': 'union',\n",
       "  'active': True,\n",
       "  '#param': 5742392,\n",
       "  'train': True,\n",
       "  '%param': 8.653018072753925},\n",
       " {'name': 'Full model', '#param': 66362880, '%param': 100.0, 'train': False}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.adapter_summary(as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2bab8decc4046a89dc62e09aa6e239d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75013 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df601b3cc9a845638ac191d7ac375e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 75013\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 8335\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "reload(processed)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.Config.TOKENIZER_NAME)\n",
    "\n",
    "tokenized_dataset= processed.tokenize_dataset(dataset,tokenizer)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128\n",
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_dataset['train'][444:490]\n",
    "\n",
    "#for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    #print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n"
     ]
    }
   ],
   "source": [
    "results = fn.group_texts(tokenized_samples, chunk_size)\n",
    "for chunk in results[\"labels\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24d32f4cb5a4853a493d2fd0c35af10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75013 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1ebd4122e34ad19c2567400a581af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 24810\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 2745\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_dataset.map(fn.group_texts, batched=True,fn_kwargs={'chunk_size': chunk_size})\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", but it was with very good reason. [SEP] [CLS] okay well i'm in east plano i'm out in las rios so it's bound to be different over here but yeah yeah [SEP] i enjoy being in las rios. [SEP] [CLS] and killed them well it was two stores side by side [SEP] it brought them to life, the fact those stores were separated. [SEP] [CLS] pickup so they did say that you had to put it out within i think it was uh three weeks after christmas and [SEP] to be picked up, it has to be placed outside 3 weeks after xmas. [SEP] [CLS] i chased a bunch of kids down on\n",
      ", but it was with very good reason. [SEP] [CLS] okay well i'm in east plano i'm out in las rios so it's bound to be different over here but yeah yeah [SEP] i enjoy being in las rios. [SEP] [CLS] and killed them well it was two stores side by side [SEP] it brought them to life, the fact those stores were separated. [SEP] [CLS] pickup so they did say that you had to put it out within i think it was uh three weeks after christmas and [SEP] to be picked up, it has to be placed outside 3 weeks after xmas. [SEP] [CLS] i chased a bunch of kids down on\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(lm_datasets['test'][1][\"input_ids\"]))\n",
    "print(tokenizer.decode(lm_datasets['test'][1]['labels']))\n",
    "print(len(lm_datasets['test'][-1]['labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] yeah [MASK] they just they never stop [MASK] things to [MASK] [SEP] they keep changing it. [SEP] [CLS] and but uh in most places yeah they got forty five kids at best i [MASK] [SEP] [MASK] got five kids at most in remove places. [SEP] [CLS] [MASK] i can get [MASK] filter off just by laying down underneath the car i can't crawl under it the clearance isn't enough but enough to [MASK] under and get the filter off [MASK]'s not a problem [SEP] there isn't enough clearance under the car for me. [SEP] [CLS] bond this house that i have [MASK] just [MASK] three bedroom ranch and i can tell you there's nothing'\n",
      "\n",
      "'>>> around it [MASK] foundation all the way round it not a shrub [MASK] a bush [SEP] i have a three bedroom ranch with no garden. all you can see is just [MASK] cement foundation all the way around. [SEP] [CLS] there's a big difference there [SEP] [MASK] is a significant [MASK] there but it works in [MASK] [MASK]. [SEP] [CLS] but [MASK] guess it automatic [MASK] guess it's automatic respective [SEP] it's automatic. [SEP] [CLS] okay okay well i [MASK] [MASK] not to [MASK] that all [unused220] from â±¼ are thugs but [MASK] two were definitely thugs and they were from brooklyn [SEP] [MASK] man says that he does not [MASK] to in bench that all people from'\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "samples = [lm_datasets['train'][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "dd=data_collator(samples)\n",
    "for chunk in dd[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] yeah they they just they never stop doing things to it [SEP] they [MASK] changing it [MASK] [SEP] [CLS] and but uh in most places yeah they [MASK] forty five kids at best i guess [SEP] they got five kids at [MASK] [MASK] those places. [SEP] [CLS] well [MASK] [MASK] get the filter off just by laying down underneath the car i can't crawl under it the clearance isn [MASK] [MASK] enough but enough to reach under and get the filter off [MASK] [MASK] s not a problem [SEP] there isn't enough clearance under the car for me. [SEP] [CLS] yeah this house that i have is just a three bedroom [MASK] and i can tell [MASK] there's [MASK]'\n",
      "\n",
      "'>>> [MASK] it cement foundation all the way round it not a shrub not a bush [SEP] i have a three bedroom ranch [MASK] no garden. all you [MASK] see is just the cement foundation all the [MASK] around. [SEP] [CLS] there's a [MASK] difference there [SEP] [MASK] is a significant difference there [MASK] it works in our favor. [SEP] [CLS] but i guess it [MASK] [MASK] guess [MASK]'[MASK] automatic respective [SEP] it's automatic [MASK] [SEP] [CLS] okay [MASK] well i'm not to say [MASK] all [MASK] [MASK] [MASK] are thugs but these two were [MASK] [MASK] and they [MASK] from [MASK] [SEP] the man says that he does not want to infer that all people from'\n"
     ]
    }
   ],
   "source": [
    "reload(fn)\n",
    "samples = [lm_datasets['train'][i] for i in range(2)]\n",
    "batch = fn.whole_word_masking_data_collator(samples,tokenizer)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_word= lambda features: fn.whole_word_masking_data_collator(features, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reload(fn)\n",
    "trainer = fn.train_mlm_model(model,adapter_name,data_collator,tokenizer, lm_datasets['train'],lm_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c82ac28a564819af9beeb7093e2b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 31.64\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4da21506aad4adcb81f100b112f357f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6348, 'learning_rate': 9.621212121212123e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8ae6402ef742cca2a1d9599408e77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.249415636062622, 'eval_runtime': 3.0564, 'eval_samples_per_second': 898.114, 'eval_steps_per_second': 28.138, 'epoch': 1.0}\n",
      "{'loss': 2.2792, 'learning_rate': 8.553719008264463e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1df664fd30f49929704d61e965c08d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1379408836364746, 'eval_runtime': 3.079, 'eval_samples_per_second': 891.523, 'eval_steps_per_second': 27.931, 'epoch': 2.0}\n",
      "{'loss': 2.2124, 'learning_rate': 7.486225895316805e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4c4dafd86e4ceaae829a957b9acf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0940940380096436, 'eval_runtime': 3.3227, 'eval_samples_per_second': 826.124, 'eval_steps_per_second': 25.882, 'epoch': 3.0}\n",
      "{'loss': 2.1759, 'learning_rate': 6.418732782369147e-05, 'epoch': 3.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5765110c67f4e7d940e00a5e79205d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.044832229614258, 'eval_runtime': 3.5122, 'eval_samples_per_second': 781.569, 'eval_steps_per_second': 24.486, 'epoch': 4.0}\n",
      "{'loss': 2.1468, 'learning_rate': 5.351239669421488e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5f098fcb064cdba9d839cc2d9b542b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0433566570281982, 'eval_runtime': 3.388, 'eval_samples_per_second': 810.217, 'eval_steps_per_second': 25.384, 'epoch': 5.0}\n",
      "{'loss': 2.1168, 'learning_rate': 4.285123966942149e-05, 'epoch': 5.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c968738c27004f83b7d188f6a50d363a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0051345825195312, 'eval_runtime': 3.3822, 'eval_samples_per_second': 811.608, 'eval_steps_per_second': 25.427, 'epoch': 6.0}\n",
      "{'loss': 2.0978, 'learning_rate': 3.217630853994491e-05, 'epoch': 6.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df45fc1fae2749e69464c09b569f11ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.029618501663208, 'eval_runtime': 3.4511, 'eval_samples_per_second': 795.399, 'eval_steps_per_second': 24.92, 'epoch': 7.0}\n",
      "{'loss': 2.0921, 'learning_rate': 2.1515151515151516e-05, 'epoch': 7.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af898f108ba4f3a9d757f18cf0269ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9852391481399536, 'eval_runtime': 3.3892, 'eval_samples_per_second': 809.915, 'eval_steps_per_second': 25.374, 'epoch': 8.0}\n",
      "{'loss': 2.0852, 'learning_rate': 1.0840220385674933e-05, 'epoch': 8.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c3af7c15774d6da164ea382198464c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9831793308258057, 'eval_runtime': 3.3692, 'eval_samples_per_second': 814.728, 'eval_steps_per_second': 25.525, 'epoch': 9.0}\n",
      "{'loss': 2.071, 'learning_rate': 1.652892561983471e-07, 'epoch': 9.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c665aea2c6d469a9968dec24765e04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9845937490463257, 'eval_runtime': 3.3793, 'eval_samples_per_second': 812.304, 'eval_steps_per_second': 25.449, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting existing adapter 'unipelt_source_mlm'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 644.0416, 'train_samples_per_second': 385.224, 'train_steps_per_second': 12.049, 'train_loss': 2.1909817385919315, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7760, training_loss=2.1909817385919315, metrics={'train_runtime': 644.0416, 'train_samples_per_second': 385.224, 'train_steps_per_second': 12.049, 'train_loss': 2.1909817385919315, 'epoch': 10.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67189c89dae48e0a2f9371d7bda5136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 7.36\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config.config as config\n",
    "\n",
    "trainer.model.save_adapter(f\"{config.Config.ADAPTER_SAVE_PATH}/{adapter_name}\", adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\"fill-mask\", model=trainer.model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> i bought the latest version of it.\n",
      ">>> i bought the latest version of that.\n",
      ">>> i bought the latest version of them.\n",
      ">>> i bought the latest version of this.\n",
      ">>> i bought the latest version of me.\n"
     ]
    }
   ],
   "source": [
    "masked_sentence = \"I bought the latest version of [MASK].\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intial-experments-_CPDD38x-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
