{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/guest/Desktop/projects/intial-experments/domain_adaptation_project/conference-experment', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages', '/home/guest/Desktop/projects/intial-experments/domain_adaptation_project/modules', '/tmp/tmpy5y5olay']\n"
     ]
    }
   ],
   "source": [
    "from setup import setup_src_path\n",
    "print(setup_src_path())\n",
    "import data.processed as processed\n",
    "import config.config as config\n",
    "import utils.setup as setup\n",
    "import utils.functions as fn\n",
    "from importlib import reload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49490eb98164b4fb11545854a8a2756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(config)\n",
    "reload(processed)\n",
    "tokenized_data,loaded_data,raw_data = processed.tokenize_and_load_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "        num_rows: 75013\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
       "        num_rows: 8335\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset= raw_data['source']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 66985530 || all params: 66985530 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel,init\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM\n",
    "\n",
    "mdlcfg = AutoConfig.from_pretrained(\n",
    "    config.Config.MODEL_NAME,\n",
    " \n",
    ")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    config.Config.MODEL_NAME,\n",
    ")\n",
    "reload(fn)\n",
    "fn.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "init(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4434048 || all params: 70827450 || trainable%: 6.2603524480974535\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from adapters import AdapterConfig, SeqBnInvConfig\n",
    "from adapters.composition import Stack\n",
    "adapter_name= \"inv_source_mlm\"\n",
    "\n",
    "adapter_config = AdapterConfig.load(\"seq_bn_inv\", reduction_factor=2, leave_out=[])\n",
    "\n",
    "model.add_adapter(adapter_name, config = adapter_config)\n",
    "#model.add_masked_lm_head(\n",
    "   # adapter_name,\n",
    " # )\n",
    "model.train_adapter(adapter_name)\n",
    "model.active_adapters = adapter_name\n",
    "\n",
    "fn.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'inv_source_mlm',\n",
       "  'architecture': 'bottleneck',\n",
       "  'active': True,\n",
       "  '#param': 3841920,\n",
       "  'train': True,\n",
       "  '%param': 5.789260502256683},\n",
       " {'name': 'Full model', '#param': 66362880, '%param': 100.0, 'train': False}]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.adapter_summary(as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 75013\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 8335\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "reload(processed)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.Config.TOKENIZER_NAME)\n",
    "\n",
    "tokenized_dataset= processed.tokenize_dataset(dataset,tokenizer)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128\n",
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_dataset['train'][444:490]\n",
    "\n",
    "#for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    #print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n",
      "'>>> Chunk length: 128'\n"
     ]
    }
   ],
   "source": [
    "results = fn.group_texts(tokenized_samples, chunk_size)\n",
    "for chunk in results[\"labels\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 24796\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 2751\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_dataset.map(fn.group_texts, batched=True,fn_kwargs={'chunk_size': chunk_size})\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll be great but until then [SEP] my husband and i can't agree on a pet. [SEP] [CLS] yeah tell me about it tell me my lucky husband's putting two of us through isn't he lucky [SEP] my husband is lucky because he is putting two of us through. [SEP] [CLS] never breaks down um the styrofoam [SEP] never destroys the styrofoam. [SEP] [CLS] i haven't seen that and i and i you know it of course won a lot of academy awards [SEP] as soon as i heard that it won a lot of academy awards, i watched it. [SEP] [CLS] and the problem is that\n",
      "ll be great but until then [SEP] my husband and i can't agree on a pet. [SEP] [CLS] yeah tell me about it tell me my lucky husband's putting two of us through isn't he lucky [SEP] my husband is lucky because he is putting two of us through. [SEP] [CLS] never breaks down um the styrofoam [SEP] never destroys the styrofoam. [SEP] [CLS] i haven't seen that and i and i you know it of course won a lot of academy awards [SEP] as soon as i heard that it won a lot of academy awards, i watched it. [SEP] [CLS] and the problem is that\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(lm_datasets['test'][1][\"input_ids\"]))\n",
    "print(tokenizer.decode(lm_datasets['test'][1]['labels']))\n",
    "print(len(lm_datasets['test'][-1]['labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] but uh we do and we have [MASK] council [MASK] that are women but you just start hearing more and more in fact oh the the governor [MASK] texas is a [MASK] too can't forget that one so [MASK] richards [SEP] there are several female counsel members that you don't [MASK] about much anymore. [SEP] [CLS] and uh its working just fine i don't have any problems at all so i'm going to keep ballot [MASK] i [MASK] offers every now [MASK] then from [SEP] i am [MASK] waiting for offers from them. [SEP] [CLS] [MASK] presumably those who find out such information if they are doing it i would prefer to not to [MASK] known and'\n",
      "\n",
      "'>>> [MASK] mean you know the classic [MASK] i don [MASK] t know cia [MASK] theories or whatever would roster [MASK] [MASK] parties trying to do it without your knowledge so there's things that invade that second type of privacy where you do [MASK] about them and possibly things that invade that second type of privacy without you knowing about it and i can [MASK] t [MASK] about the second one other than to to to generate paranoia yeah to surmise and [MASK]'d like [MASK] think that's it's quite [MASK] at [MASK] in this country i [MASK]'t feel like the kgb is monitoring my phone or anything like that [SEP] [MASK] fbi is right behind me. [SEP]'\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "samples = [lm_datasets['train'][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "dd=data_collator(samples)\n",
    "for chunk in dd[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] but [MASK] we do and we have [MASK] council women that [MASK] women but you just start hearing more and more in fact oh the the governor of texas is a woman too [MASK] [MASK] t forget that one so ann richards [SEP] there [MASK] several female counsel members that you [MASK]'[MASK] [MASK] about [MASK] anymore. [SEP] [CLS] and uh its working just fine i don't have [MASK] problems at all so i'm going to keep [MASK] uh i get offers every now and [MASK] from [SEP] i am still waiting for offers from them. [SEP] [CLS] well presumably those who find [MASK] such information if they are doing it [MASK] would prefer [MASK] not to [MASK] known and'\n",
      "\n",
      "'>>> [MASK] mean you know the classic [MASK] i don't know cia [MASK] theories or whatever would [MASK] [MASK] such parties trying to do it without your knowledge [MASK] there's [MASK] that invade that second type of privacy where you do know about them and possibly things that invade that second type of privacy [MASK] you knowing about [MASK] [MASK] i can't talk about the [MASK] [MASK] other than to to [MASK] [MASK] paranoia yeah to surmise [MASK] i'd like to think that's it's quite low at least in this country i don'[MASK] feel [MASK] the kgb is monitoring my phone or anything like that [SEP] the [MASK] [MASK] [MASK] behind me [MASK] [SEP]'\n"
     ]
    }
   ],
   "source": [
    "reload(fn)\n",
    "samples = [lm_datasets['train'][i] for i in range(2)]\n",
    "batch = fn.whole_word_masking_data_collator(samples,tokenizer)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator_word= lambda features: fn.whole_word_masking_data_collator(features, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reload(fn)\n",
    "trainer = fn.train_mlm_model(model,\"mlm_with_inv\",data_collator,tokenizer, lm_datasets['train'],lm_datasets['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee2efab5ce449f6af7906772f5bf6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 33.92\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114514fef75d4e538b7171ce3f776cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5837, 'learning_rate': 9.622068965517242e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eccd70bf68041469def2c4927d52c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2370874881744385, 'eval_runtime': 3.0156, 'eval_samples_per_second': 912.261, 'eval_steps_per_second': 28.519, 'epoch': 1.0}\n",
      "{'loss': 2.2602, 'learning_rate': 8.55448275862069e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f69eb4034c4e37b9f9879d9ad2a840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1107497215270996, 'eval_runtime': 3.0203, 'eval_samples_per_second': 910.847, 'eval_steps_per_second': 28.474, 'epoch': 2.0}\n",
      "{'loss': 2.1636, 'learning_rate': 7.486896551724138e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f63d0736eb34d0ebaaff482b8764c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.06278395652771, 'eval_runtime': 3.0162, 'eval_samples_per_second': 912.079, 'eval_steps_per_second': 28.513, 'epoch': 3.0}\n",
      "{'loss': 2.1185, 'learning_rate': 6.419310344827587e-05, 'epoch': 3.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1be4328e4054313bec9b695883a6d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0210649967193604, 'eval_runtime': 3.0202, 'eval_samples_per_second': 910.876, 'eval_steps_per_second': 28.475, 'epoch': 4.0}\n",
      "{'loss': 2.0863, 'learning_rate': 5.351724137931035e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cbfc6a4dd84d7d947bb9e49547f8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0022971630096436, 'eval_runtime': 3.0303, 'eval_samples_per_second': 907.834, 'eval_steps_per_second': 28.38, 'epoch': 5.0}\n",
      "{'loss': 2.0632, 'learning_rate': 4.284137931034483e-05, 'epoch': 5.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3914ebb35b34559baf48cdb2db54d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9874132871627808, 'eval_runtime': 3.0468, 'eval_samples_per_second': 902.91, 'eval_steps_per_second': 28.226, 'epoch': 6.0}\n",
      "{'loss': 2.04, 'learning_rate': 3.216551724137931e-05, 'epoch': 6.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2249fc53e9b6492ab6c569ac216d77ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.958559513092041, 'eval_runtime': 3.0685, 'eval_samples_per_second': 896.537, 'eval_steps_per_second': 28.027, 'epoch': 7.0}\n",
      "{'loss': 2.0314, 'learning_rate': 2.150344827586207e-05, 'epoch': 7.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fff6ce59df047fb90448b7d32163b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9335156679153442, 'eval_runtime': 3.0536, 'eval_samples_per_second': 900.893, 'eval_steps_per_second': 28.163, 'epoch': 8.0}\n",
      "{'loss': 2.0158, 'learning_rate': 1.0827586206896552e-05, 'epoch': 8.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8893af6bd2cb48e0ba9fbc20850785c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.947245717048645, 'eval_runtime': 3.0533, 'eval_samples_per_second': 900.991, 'eval_steps_per_second': 28.166, 'epoch': 9.0}\n",
      "{'loss': 1.9985, 'learning_rate': 1.5172413793103449e-07, 'epoch': 9.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d42a3156eb468d81af2a359f023d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting existing adapter 'inv_source_mlm'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9326860904693604, 'eval_runtime': 3.0308, 'eval_samples_per_second': 907.668, 'eval_steps_per_second': 28.375, 'epoch': 10.0}\n",
      "{'train_runtime': 547.6569, 'train_samples_per_second': 452.765, 'train_steps_per_second': 14.151, 'train_loss': 2.1359461369668282, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7750, training_loss=2.1359461369668282, metrics={'train_runtime': 547.6569, 'train_samples_per_second': 452.765, 'train_steps_per_second': 14.151, 'train_loss': 2.1359461369668282, 'epoch': 10.0})"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118efdb4756b435da1033f51e4d83c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 7.03\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config.config as config\n",
    "\n",
    "trainer.model.save_adapter(f\"{config.Config.ADAPTER_SAVE_PATH}/{adapter_name}\", adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      7\u001b[0m mask_filler \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfill-mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\"fill-mask\", model=trainer.model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> i cancelled my job.\n",
      ">>> i cancelled my vacation.\n",
      ">>> i cancelled my plans.\n",
      ">>> i cancelled my trip.\n",
      ">>> i cancelled my flight.\n"
     ]
    }
   ],
   "source": [
    "masked_sentence = \"I bought a [MASK].\"\n",
    "preds = mask_filler(masked_sentence)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intial-experments-_CPDD38x-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
