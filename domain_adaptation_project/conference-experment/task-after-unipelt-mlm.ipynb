{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/guest/Desktop/projects/intial-experments/domain_adaptation_project/conference-experment', '/usr/lib/python38.zip', '/usr/lib/python3.8', '/usr/lib/python3.8/lib-dynload', '', '/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages', '/home/guest/Desktop/projects/intial-experments/domain_adaptation_project/modules', '/tmp/tmphhta90ja']\n"
     ]
    }
   ],
   "source": [
    "from setup import setup_src_path\n",
    "print(setup_src_path())\n",
    "import data.processed as processed\n",
    "import config.config as config\n",
    "import utils.setup as setup\n",
    "import utils.functions as fn\n",
    "from importlib import reload\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "source_data=load_from_disk(f\"{config.Config.DATASETS_SAVE_PATH}/source_data\")\n",
    "source_data_eval=load_from_disk(f\"{config.Config.DATASETS_SAVE_PATH}/source_data_eval\")\n",
    "target_data=load_from_disk(f\"{config.Config.DATASETS_SAVE_PATH}/target_data\")\n",
    "target_data_eval=load_from_disk(f\"{config.Config.DATASETS_SAVE_PATH}/target_data_eval\")\n",
    "test_target_data=load_from_disk(f\"{config.Config.DATASETS_SAVE_PATH}/test_target_data\")\n",
    "unsupervised_target=load_from_disk(f\"{config.Config.DATASETS_SAVE_PATH}/unsupervised_target\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "source_data_loader = DataLoader(source_data, batch_size=32, shuffle=True)\n",
    "target_data_loader = DataLoader(target_data, batch_size=32)\n",
    "source_data_eval_loader = DataLoader(source_data_eval, batch_size=32, shuffle=True)\n",
    "target_data_eval_loader = DataLoader(target_data_eval, batch_size=32)\n",
    "\n",
    "target_test_loader = DataLoader(test_target_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 1976\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 66985530 || all params: 66985530 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoConfig\n",
    "\n",
    "mdlcfg = AutoConfig.from_pretrained(\n",
    "    config.Config.MODEL_NAME,\n",
    " \n",
    ")\n",
    "model = AutoAdapterModel.from_pretrained(\n",
    "    config.Config.MODEL_NAME,\n",
    "    config=mdlcfg,\n",
    ")\n",
    "reload(fn)\n",
    "fn.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from adapters import AdapterConfig\n",
    "from adapters.composition import Stack\n",
    "\n",
    "adapter_name= \"task_adapter_after_unipelt_mlm\"\n",
    "lang_adapter_config = AdapterConfig.load(\"unipelt\", reduction_factor=2)\n",
    "model.load_adapter(f\"{config.Config.ADAPTER_SAVE_PATH}/unipelt_source_mlm\", config=lang_adapter_config)\n",
    "model.load_adapter(f\"{config.Config.ADAPTER_SAVE_PATH}/unipelt_target_mlm\", config=lang_adapter_config)\n",
    "model.add_adapter(adapter_name, config=\"unipelt\")\n",
    "model.add_classification_head(\n",
    "    adapter_name,\n",
    "    num_labels=3,\n",
    "  )\n",
    "model.train_adapter([adapter_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8203241 || all params: 86050905 || trainable%: 9.533009559864594\n"
     ]
    }
   ],
   "source": [
    "model.active_adapters = Stack(\"unipelt_source_mlm\", adapter_name)\n",
    "\n",
    "fn.print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'unipelt_source_mlm',\n",
       "  'architecture': 'union',\n",
       "  'active': True,\n",
       "  '#param': 5742392,\n",
       "  'train': False,\n",
       "  '%param': 8.653018072753925},\n",
       " {'name': 'unipelt_target_mlm',\n",
       "  'architecture': 'union',\n",
       "  'active': False,\n",
       "  '#param': 5742392,\n",
       "  'train': False,\n",
       "  '%param': 8.653018072753925},\n",
       " {'name': 'task_adapter_after_unipelt_mlm',\n",
       "  'architecture': 'union',\n",
       "  'active': True,\n",
       "  '#param': 5742392,\n",
       "  'train': True,\n",
       "  '%param': 8.653018072753925},\n",
       " {'name': 'Full model', '#param': 66362880, '%param': 100.0, 'train': False}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.adapter_summary(as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 75013\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, EvalPrediction,default_data_collator\n",
    "from adapters import AdapterTrainer\n",
    "eval_data = None\n",
    "batch_size = 32\n",
    "\n",
    "logging_steps = len(source_data) // batch_size\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        \n",
    "        output_dir=f\"{config.Config.RESULTS_SAVE_PATH}/task_after_mlm/results\",                 # Where to store the output (checkpoints and predictions)\n",
    "        num_train_epochs=8,                     # Total number of training epochs\n",
    "        per_device_train_batch_size=batch_size,         # Batch size for training\n",
    "        per_device_eval_batch_size=batch_size,          # Batch size for evaluation\n",
    "        warmup_steps=500,                       # Number of warmup steps for learning rate scheduler\n",
    "        learning_rate=1e-4,\n",
    "        weight_decay=0.01,                      # Strength of weight decay\n",
    "        logging_dir=f\"{config.Config.RESULTS_SAVE_PATH}/task_after_mlm/logs\",                   # Directory for storing logs\n",
    "        logging_steps=logging_steps,                       # Log every X updates steps\n",
    "        remove_unused_columns=False,\n",
    "        evaluation_strategy=\"steps\" if eval_data is not None else \"no\",            # Evaluate model every X steps\n",
    "        eval_steps=logging_steps,                         # Number of steps to perform evaluation\n",
    "        save_steps=logging_steps,                         # Save checkpoint every X steps\n",
    "        save_total_limit=2,                     # Limit the total amount of checkpoints\n",
    "        load_best_model_at_end=True if eval_data is not None else False,            # Load the best model when finished training\n",
    "        report_to=\"none\"                        # Do not report to any online service\n",
    "    ) \n",
    "trainer = AdapterTrainer(\n",
    "        model=model,                           # The instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,                    # Training arguments, defined above\n",
    "        train_dataset=source_data,           # Training dataset\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9228e941bfd74e60bb2bc619ba064a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.841, 'learning_rate': 8.990142387732749e-05, 'epoch': 1.0}\n",
      "{'loss': 0.6579, 'learning_rate': 7.706462212486309e-05, 'epoch': 2.0}\n",
      "{'loss': 0.6087, 'learning_rate': 6.422782037239868e-05, 'epoch': 3.0}\n",
      "{'loss': 0.5771, 'learning_rate': 5.1391018619934285e-05, 'epoch': 4.0}\n",
      "{'loss': 0.5518, 'learning_rate': 3.855421686746988e-05, 'epoch': 5.0}\n",
      "{'loss': 0.5322, 'learning_rate': 2.5717415115005478e-05, 'epoch': 6.0}\n",
      "{'loss': 0.5137, 'learning_rate': 1.2880613362541074e-05, 'epoch': 7.0}\n",
      "{'loss': 0.5017, 'learning_rate': 4.381161007667032e-08, 'epoch': 8.0}\n",
      "{'train_runtime': 2468.2587, 'train_samples_per_second': 243.128, 'train_steps_per_second': 7.6, 'train_loss': 0.5980198588960969, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18760, training_loss=0.5980198588960969, metrics={'train_runtime': 2468.2587, 'train_samples_per_second': 243.128, 'train_steps_per_second': 7.6, 'train_loss': 0.5980198588960969, 'epoch': 8.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No adapter with name 'union_target_mlm' found. Please make sure that all specified adapters are correctly loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapters\u001b[49m \u001b[38;5;241m=\u001b[39m Stack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munion_target_mlm\u001b[39m\u001b[38;5;124m\"\u001b[39m, adapter_name)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39madapter_summary(as_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1745\u001b[0m     buffers[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/adapters/model_mixin.py:506\u001b[0m, in \u001b[0;36mModelAdaptersMixin.active_adapters\u001b[0;34m(self, adapter_setup)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;129m@active_adapters\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactive_adapters\u001b[39m(\u001b[38;5;28mself\u001b[39m, adapter_setup: Union[\u001b[38;5;28mlist\u001b[39m, AdapterCompositionBlock]):\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_active_adapters\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_setup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/adapters/heads/model_mixin.py:240\u001b[0m, in \u001b[0;36mModelWithFlexibleHeadsAdaptersMixin.set_active_adapters\u001b[0;34m(self, adapter_setup, skip_layers)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_active_adapters\u001b[39m(\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m, adapter_setup: Union[\u001b[38;5;28mlist\u001b[39m, AdapterCompositionBlock], skip_layers: Optional[List[\u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    228\u001b[0m ):\n\u001b[1;32m    229\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    Sets the adapter modules to be used by default in every forward pass. This setting can be overriden by passing\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    the `adapter_names` parameter in the `foward()` pass. If no adapter with the given name is found, no module of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m            The list of adapters to be activated by default. Can be a fusion or stacking configuration.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_active_adapters\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_setup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# use last adapter name as name of prediction head\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/adapters/model_mixin.py:526\u001b[0m, in \u001b[0;36mModelAdaptersMixin.set_active_adapters\u001b[0;34m(self, adapter_setup, skip_layers)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m adapter_name \u001b[38;5;129;01min\u001b[39;00m adapter_setup\u001b[38;5;241m.\u001b[39mflatten():\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapters_config\u001b[38;5;241m.\u001b[39madapters:\n\u001b[0;32m--> 526\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo adapter with name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00madapter_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m found. Please make sure that all specified adapters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are correctly loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m             )\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# Make sure LoRA is reset\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_adapter()\n",
      "\u001b[0;31mValueError\u001b[0m: No adapter with name 'union_target_mlm' found. Please make sure that all specified adapters are correctly loaded."
     ]
    }
   ],
   "source": [
    "model.active_adapters = Stack(\"union_target_mlm\", adapter_name)\n",
    "\n",
    "model.adapter_summary(as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/.cache/pypoetry/virtualenvs/intial-experments-_CPDD38x-py3.8/lib/python3.8/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53a22b2ba3f4399b19da402a37f2535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/247 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7744819521903992,\n",
       " 'eval_accuracy': 0.6928137651821862,\n",
       " 'eval_f1': 0.695254146957065,\n",
       " 'eval_precision': 0.7420792560664218,\n",
       " 'eval_recall': 0.6928137651821862,\n",
       " 'eval_runtime': 4.6968,\n",
       " 'eval_samples_per_second': 420.711,\n",
       " 'eval_steps_per_second': 52.589}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import EvalPrediction\n",
    "from sklearn.metrics import accuracy_score, f1_score,precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def compute_metrics(pred:EvalPrediction):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "eval_trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(output_dir=\"./eval_output\", remove_unused_columns=False,),\n",
    "    eval_dataset=test_target_data,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "eval_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config.config as config\n",
    "\n",
    "trainer.model.save_adapter(f\"{config.Config.ADAPTER_SAVE_PATH}/{adapter_name}\", adapter_name,with_head=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intial-experments-_CPDD38x-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
